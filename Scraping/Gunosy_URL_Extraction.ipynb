{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T13:33:41.394849Z",
     "start_time": "2018-12-21T13:33:40.994097Z"
    }
   },
   "source": [
    "# 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. トップページからカテゴリ別のURLを抽出=> **dic**\n",
    "\n",
    "1.　カテゴリのトップページからリンク(次のページ)のURLを抽出 => **dic**\n",
    "\n",
    "1. カテゴリごとに全抽出済みのURLから記事を読む "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリ import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T14:20:17.242635Z",
     "start_time": "2018-12-21T14:20:17.240002Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T14:19:14.001052Z",
     "start_time": "2018-12-21T14:19:13.988582Z"
    }
   },
   "outputs": [],
   "source": [
    "# トップページからカテゴリ別に分けられてるページのURLを抽出\n",
    "def main_url_func(main_url):\n",
    "    res = urllib.request.urlopen(main_url)\n",
    "    data = res.read()\n",
    "    text = data.decode(\"utf-8\")\n",
    "    soup = BS(text, 'html.parser')\n",
    "    ul = soup.html.body.nav.ul\n",
    "    li = ul.find_all('li', class_=re.compile(r'^(nav_color_).$'),)\n",
    "\n",
    "    for i in li:    \n",
    "        main_url_dic[i.a.string] = i.a.attrs['href']\n",
    "    return main_url_dic\n",
    "\n",
    "\n",
    "# カテゴリごとのページを訪れ、次のページへのリンクを抽出\n",
    "def same_category_other_page_url(main_dic):\n",
    "\n",
    "    for category, start_url  in main_dic.items():\n",
    "        other_pages_list = []\n",
    "    \n",
    "        url = start_url\n",
    "        res = urllib.request.urlopen(url)\n",
    "        data = res.read()\n",
    "        text = data.decode(\"utf-8\")\n",
    "        soup = BS(text, \"html.parser\")\n",
    "    \n",
    "        body = soup.html.body\n",
    "        li = body.find_all(\"a\", href=re.compile(r'^/categories/1.'))\n",
    "    \n",
    "        for i in li:\n",
    "            other_pages_list.append(\"https://gunosy.com\" + i.attrs['href'])\n",
    "            \n",
    "        main_dic[category] = other_pages_list\n",
    "    return main_dic\n",
    "\n",
    "\n",
    "#　カテゴリ別のページにアクセスして記事のあるURLを抽出\n",
    "def categpry_url_func(main_dic):\n",
    "    sub_url_dic = {}\n",
    "    \n",
    "    for category, url in main_dic.items():\n",
    "        url_list = []  \n",
    "        res = urllib.request.urlopen(url)\n",
    "        data = res.read()\n",
    "        text = data.decode(\"utf-8\")\n",
    "        soup = BS(text, \"html.parser\")\n",
    "        soup_html = soup.html.body\n",
    "        li = soup_html.find_all(\"a\", href=re.compile(r'^https://gunosy.com/articles/'))\n",
    "    \n",
    "        for i in li:\n",
    "            url_list.append(i.attrs['href'])\n",
    "        \n",
    "        sub_url_dic[category] = url_list\n",
    "    \n",
    "    return sub_url_dic\n",
    "\n",
    "# 記事のtextをスクレイピング\n",
    "def text_scraping(url_dic):\n",
    "    for category, li in url_dic.items():\n",
    "        for url in li:\n",
    "            res = urllib.request.urlopen(url)\n",
    "            data = res.read()\n",
    "            text = data.decode(\"utf-8\")\n",
    "            soup = BS(text, \"html.parser\")\n",
    "            body = soup.html.body\n",
    "            articlie = body.find_all(\"p\")\n",
    "            for i in articlie:\n",
    "                print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T14:18:36.142384Z",
     "start_time": "2018-12-21T14:18:35.117976Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "グノシー\n",
      "我が道を行く新感覚グルメドラマ『忘却のサチコ』（テレビ東京系）。第10話では、あの有名店のオムライスに幸子がほだされます。\n",
      "None\n",
      "■「いつも通り＝精一杯努める」の幸子\n",
      "編集者によるネット討論番組に出演することになった我らが佐々木幸子（高畑充希）。その名も「今必要な恋愛小説」。\n",
      "編集長（吹越満）から「いつも通りでいいからな」と言われるも「はい、精一杯努めてまいります！」と高らかに返事する幸子。確かに、いつでも愚直に手抜きなしが幸子の「いつも通り」。編集長はおそらくリラックスさせる意図で言ったのだろうが、そう受け取れない不器用な幸子がかっこいい。\n",
      "■まずは「おやき」で忘却\n",
      "しかし本番前、ネット番組→全世界の人が見ている→結婚式当日に失踪した元・新郎の俊吾さん（早乙女太一）が生放送中に現れるかも！　と妄想し、パニックになりかけた幸子は、いつものように美味しいものを食べてつらい気持ちを「忘却」することに。\n",
      "幸子の目に留まったのは、主催者が長野出身とのことでケータリングに置かれていた「おやき」。\n",
      "言わずと知れた、きんぴらやかぼちゃなどの具を小麦の皮で包んで焼いたお惣菜饅頭。\n",
      "幸子が選んだのは、定番中の定番の具材、野沢菜。\n",
      "奇をてらってない飽きの来ない味は、いつ食べても落ち着く。\n",
      "蒸した肉まんよりやや皮が硬めなのも、武骨でいい。素朴を絵に描いたような郷土の味ながら、現代でも通じる惣菜パン的おやつ。\n",
      "■幸子のライバル\n",
      "実はこの討論番組のキャスティングは、「月刊スピカ」の尾野真由美（佐藤めぐみ）が裏で手を回し仕組んだもの。\n",
      "8話で初登場し、接待の場で幸子に大敗を喫した尾野は、幸子を陥れるために復讐の炎を燃やす、わかりやすい女狐タイプのライバルだ。\n",
      "今回も本番前に主催者で司会の社会学者（六角慎司）の手をハプニングを装って握り、色仕掛けに走るなど余念がないが、幸子にストッキングが伝線してることを教えられパニクるなどやはり天敵・幸子にペースを乱される。\n",
      "本番中も、社会問題に恋愛を絡めたような作品、読者の知識レベルや倫理観を高められるような小説が必要、毒にも薬にもならない小説は必要ないと強弁し、存在感を示そうと躍起になる尾野。\n",
      "それに対し、社会的弱者であろうとなかろうと悩みはさまざまなので、自分の物語だと思える作品を見つけることができるのが「小説」であるから、よって「今必要な恋愛小説」の答えは「全ての恋愛小説」だとある種の正解を出してしまう幸子。\n",
      "さらにネット住民が喜びそうな作家と編集者との色恋話をあえて繰り出す尾野に対し、幸子は、小説家とは身体一つで真っ暗な宇宙（未知の世界）へ飛び込んで人類（読者）を新しい世界へと誘うパイオニア「はやぶさ2号」（小惑星探査機）であると、壮大な例えを繰り出す。\n",
      "当初そんな存在と恋愛はできないとしていた幸子だが、悩んだ挙句、作家の先生を尊敬しているから、「もし愛する人が作家になったら愛と尊敬を両立させ新しい感情の扉を開くことができるかもしれません」と気持ちを吐き出す。\n",
      "尾野が常に視聴者や世論など他者のウケを優先し、ある種、媚びて発言するのに対し、幸子の発言は無茶苦茶ながらも自分の想いが貫かれており、そこに嘘がない。\n",
      "■幸子は嘘をつかない\n",
      "そう、この討論に限らず幸子は嘘をつかないのだ。\n",
      "いや、つけないと言ってもいいのかもしれない。\n",
      "式の最中に新郎がいなくなった時も、逃げられた悲劇の新婦である以上に、責任を伴う立場の当事者として、自ら状況を正直に淡々と説明、最悪の状況を「以上です」で締めくくり参列者を驚かせた。\n",
      "常に嘘がない故に、空気を読むことに長けまくっている現代の世では奇特に見えてしまうことも多いが、そのひたむきさにどこか胸を打たれることも多い。\n",
      "他人どころか自らを騙すなど自分に嘘がつけないから、毎度逃げた俊吾さんを思い出しては苦しんでしまうのだろう。\n",
      "当初は幸子、というか高畑のコスプレが必ずあったのだが、最近は俊吾さんこと早乙女太一のコスプレに移行してきている。\n",
      "今回も、宇宙から来た正義のヒーローや和装の文豪に早乙女が扮しているが、借り物的なコスプレではなくかなりの熱演で、早乙女の新たな魅力を見せてくれている。\n",
      "■三代目登場！\n",
      "番組終了後、幸子が立ち寄った店はあの「たいめいけん」。\n",
      "オムライスと真っ黒な3代目店長でお馴染みのあの老舗洋食店。\n",
      "絶品のカニクリームコロッケやコンソメスープを味わったあと、いよいよ名物タンポポオムライスが到着。\n",
      "伊丹十三の映画『たんぽぽ』（1985）でもお馴染みの、切り開くととろとろの卵が溢れるあのオムライス。\n",
      "デミグラスソースを纏った卵とチキンライスを口にいれ、思わず目を細める幸子。\n",
      "ケチャップもいいが洋食店ならではのデミグラスもいい。\n",
      "店長と見分けがつかないくらいの漆黒が卵に映える。\n",
      "食べてる最中、第2話のように3代目と踊り出すのでは？　とソワソワしたが、無事そういうハプニングもなく終了。3代目の不自然な固い笑顔がよかった。\n",
      "ちなみに映画『たんぽぽ』で、このオムライスが登場した時は、デミグラスではなく真っ赤なケチャップをかけている。\n",
      "トマトが好物な高畑にはそちらも食べさせてみたかった。\n",
      "■脇役の宝庫\n",
      "結局この日も幸子にいいところを持っていかれ、またしてもグギギとなった尾野真由美だが、だんだんその純粋な負けん気が可愛く見えてくるから不思議だ。\n",
      "そして、ニコニコ動画らしきサイトで「佐々木さん頑張れ」と必死にコメントを連打する後輩・小林（葉山奨之）も、どんどん存在感を増している。\n",
      "そして、堂々としてるがゆえに見落としがちだが、小林より先に「佐々木頑張れ」とコメントを打とうと言い出し、実践していた編集長。\n",
      "原作でもそうだが、時折佐々木を想う場面が見られる（恋愛感情かどうかは定かでないが）ので、もっと編集長を活躍させてあげてほしい。\n",
      "残りあとわずか。\n",
      "None\n",
      "元記事を読む\n",
      "None\n",
      "『忘却のサチコ』に“ジーニアス黒田”再び！　踊る高畑充希に食う高畑充希……\n",
      "『忘却のサチコ』サワークリームのような甘酸っぱい葉山奨之に母性を刺激される……今回はロシア料理！\n",
      "『忘却のサチコ』グルメドラマなのに展開が気になるなんて……ついに本物の“俊吾さん”が登場！\n",
      "最近人気のあったニュース\n",
      "JOY、「絶縁宣言」の相手は「ZOZO・前澤氏のモノマネ芸人」!? “炎上商法”に周囲呆れ\n",
      "小嶋陽菜“一般人が撮った写真”にあぜん...芸能界固執が痛々しい\n",
      "「僕、韓国人だったんですよ」実は日本人じゃない意外な芸能人たち\n",
      "2億円整形サイボーグ・ヴァニラの現在に悲鳴 「怖すぎる」「命が危なそう」\n",
      "松嶋尚美、南青山の児童相談所建設への発言に視聴者愕然 「虐待」「偏見であり差別」と炎上\n",
      "人気声優・平野綾(31)の現在に衝撃 「マジかよ...」「あり得ない」と視聴者驚愕\n",
      "元TOKIO・山口達也氏を見舞う唯一のメンバーとは... 「想像して涙出た」と感動拡がる\n",
      "「胸とお尻の二刀流」...鈴木ふみ奈、むっちりセクシーなグラビア公開！\n",
      "どんぎつね・吉岡里穂の1st写真集「ぼくのそばにきみがいる」が公開！\n",
      "浜崎あゆみ(40)コスプレ姿が披露される 「これは問題」「顔面偏差値が...」\n",
      "キーワードで気になるニュースを絞りこもう\n"
     ]
    }
   ],
   "source": [
    "url = 'https://gunosy.com/articles/Rp4x4'\n",
    "text_scraping({\"a\":[url]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
